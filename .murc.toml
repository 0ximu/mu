# MU Configuration
# https://github.com/dominaite/mu

[mu]
version = "1.0"

[scanner]
ignore = [
    "node_modules/",
    ".git/",
    "__pycache__/",
    "*.pyc",
    ".venv/",
    "venv/",
    "dist/",
    "build/",
    "target/",    # Rust build output
    "archive/",   # MU archive directory
    "*.min.js",
    "*.bundle.js",
    "*.lock",
    ".mu/",       # MU data directory
]
include_hidden = false
max_file_size_kb = 1000

[parser]
# "auto" = detect from file extensions
# Or specify: ["python", "typescript", "csharp"]
languages = "auto"

[reducer]
strip_comments = true
strip_docstrings = false  # Keep for semantic value
complexity_threshold = 20  # AST nodes before LLM summarization

[llm]
enabled = false  # Set to true to enable LLM summarization
provider = "anthropic"  # anthropic | openai | ollama | openrouter
model = "claude-3-haiku-20240307"
timeout_seconds = 30
max_retries = 2

[llm.ollama]
base_url = "http://localhost:11434"
model = "codellama"

[security]
redact_secrets = true
secret_patterns = "default"  # Or path to custom patterns file

[output]
format = "mu"  # mu | json | markdown
include_line_numbers = false
include_file_hashes = true
shell_safe = false

[cache]
enabled = true
directory = ".mu/cache"  # Cache within .mu/ directory
ttl_hours = 168  # 1 week

[embeddings]
provider = "openai"  # openai | local
batch_size = 100
cache_embeddings = true

[embeddings.openai]
api_key_env = "OPENAI_API_KEY"
model = "text-embedding-3-small"

[embeddings.local]
model = "all-MiniLM-L6-v2"
device = "auto"  # auto | cpu | cuda | mps
